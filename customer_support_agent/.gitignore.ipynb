{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3683abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9518107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal\n",
    "\n",
    "# Define the structure for email classification\n",
    "class EmailClassification(TypedDict):\n",
    "    intent: Literal[\"question\", \"bug\", \"billing\", \"feature\", \"complex\"]\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n",
    "    topic: str\n",
    "    summary: str\n",
    "\n",
    "class EmailAgentState(TypedDict):\n",
    "    # Raw email data\n",
    "    email_content: str\n",
    "    sender_email: str\n",
    "    email_id: str\n",
    "\n",
    "    # Classification result\n",
    "    classification: EmailClassification | None\n",
    "\n",
    "    # Raw search/API results\n",
    "    search_results: list[str] | None  # List of raw document chunks\n",
    "    customer_history: dict | None  # Raw customer data from CRM\n",
    "\n",
    "    # Generated content\n",
    "    draft_response: str | None\n",
    "    messages: list[str] | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f067e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import interrupt, Command, RetryPolicy\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.messages import HumanMessage\n",
    "import os,dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    temperature=0\n",
    "\n",
    ")\n",
    "def read_email(state: EmailAgentState) -> dict:\n",
    "    \"\"\"Extract and parse email content\"\"\"\n",
    "    # In production, this would connect to your email service\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=f\"Processing email: {state['email_content']}\")]\n",
    "    }\n",
    "\n",
    "def classify_intent(state: EmailAgentState) -> Command[Literal[\"search_documentation\", \"human_review\", \"draft_response\", \"bug_tracking\"]]:\n",
    "    \"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\"\n",
    "\n",
    "    # Create structured LLM that returns EmailClassification dict\n",
    "    structured_llm = llm.with_structured_output(EmailClassification)\n",
    "\n",
    "    # Format the prompt on-demand, not stored in state\n",
    "    classification_prompt = f\"\"\"\n",
    "    Analyze this customer email and classify it:\n",
    "\n",
    "    Email: {state['email_content']}\n",
    "    From: {state['sender_email']}\n",
    "\n",
    "    Provide classification including intent, urgency, topic, and summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get structured response directly as dict\n",
    "    classification = structured_llm.invoke(classification_prompt)\n",
    "\n",
    "    # Determine next node based on classification\n",
    "    if classification['intent'] == 'billing' or classification['urgency'] == 'critical':\n",
    "        goto = \"human_review\"\n",
    "    elif classification['intent'] in ['question', 'feature']:\n",
    "        goto = \"search_documentation\"\n",
    "    elif classification['intent'] == 'bug':\n",
    "        goto = \"bug_tracking\"\n",
    "    else:\n",
    "        goto = \"draft_response\"\n",
    "\n",
    "    # Store classification as a single dict in state\n",
    "    return Command(\n",
    "        update={\"classification\": classification},\n",
    "        goto=goto\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d26782c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documentation(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n",
    "    \"\"\"Search knowledge base for relevant information\"\"\"\n",
    "\n",
    "    # Build search query from classification\n",
    "    classification = state.get('classification', {})\n",
    "    query = f\"{classification.get('intent', '')} {classification.get('topic', '')}\"\n",
    "\n",
    "    try:\n",
    "        # Implement your search logic here\n",
    "        # Store raw search results, not formatted text\n",
    "        search_results = [\n",
    "            \"Reset password via Settings > Security > Change Password\",\n",
    "            \"Password must be at least 12 characters\",\n",
    "            \"Include uppercase, lowercase, numbers, and symbols\"\n",
    "        ]\n",
    "    except SearchAPIError as e:\n",
    "        # For recoverable search errors, store error and continue\n",
    "        search_results = [f\"Search temporarily unavailable: {str(e)}\"]\n",
    "\n",
    "    return Command(\n",
    "        update={\"search_results\": search_results},  # Store raw results or error\n",
    "        goto=\"draft_response\"\n",
    "    )\n",
    "\n",
    "def bug_tracking(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\n",
    "    \"\"\"Create or update bug tracking ticket\"\"\"\n",
    "\n",
    "    # Create ticket in your bug tracking system\n",
    "    ticket_id = \"BUG-12345\"  # Would be created via API\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"search_results\": [f\"Bug ticket {ticket_id} created\"],\n",
    "            \"current_step\": \"bug_tracked\"\n",
    "        },\n",
    "        goto=\"draft_response\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5117c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_response(state: EmailAgentState) -> Command[Literal[\"human_review\", \"send_reply\"]]:\n",
    "    \"\"\"Generate response using context and route based on quality\"\"\"\n",
    "\n",
    "    classification = state.get('classification', {})\n",
    "\n",
    "    # Format context from raw state data on-demand\n",
    "    context_sections = []\n",
    "\n",
    "    if state.get('search_results'):\n",
    "        # Format search results for the prompt\n",
    "        formatted_docs = \"\\n\".join([f\"- {doc}\" for doc in state['search_results']])\n",
    "        context_sections.append(f\"Relevant documentation:\\n{formatted_docs}\")\n",
    "\n",
    "    if state.get('customer_history'):\n",
    "        # Format customer data for the prompt\n",
    "        context_sections.append(f\"Customer tier: {state['customer_history'].get('tier', 'standard')}\")\n",
    "\n",
    "    # Build the prompt with formatted context\n",
    "    draft_prompt = f\"\"\"\n",
    "    Draft a response to this customer email:\n",
    "    {state['email_content']}\n",
    "\n",
    "    Email intent: {classification.get('intent', 'unknown')}\n",
    "    Urgency level: {classification.get('urgency', 'medium')}\n",
    "\n",
    "    {chr(10).join(context_sections)}\n",
    "\n",
    "    Guidelines:\n",
    "    - Be professional and helpful\n",
    "    - Address their specific concern\n",
    "    - Use the provided documentation when relevant\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(draft_prompt)\n",
    "\n",
    "    # Determine if human review needed based on urgency and intent\n",
    "    needs_review = (\n",
    "        classification.get('urgency') in ['high', 'critical'] or\n",
    "        classification.get('intent') == 'complex'\n",
    "    )\n",
    "\n",
    "    # Route to appropriate next node\n",
    "    goto = \"human_review\" if needs_review else \"send_reply\"\n",
    "\n",
    "    return Command(\n",
    "        update={\"draft_response\": response.content},  # Store only the raw response\n",
    "        goto=goto\n",
    "    )\n",
    "\n",
    "def human_review(state: EmailAgentState) -> Command[Literal[\"send_reply\", END]]:\n",
    "    \"\"\"Pause for human review using interrupt and route based on decision\"\"\"\n",
    "\n",
    "    classification = state.get('classification', {})\n",
    "\n",
    "    # interrupt() must come first - any code before it will re-run on resume\n",
    "    human_decision = interrupt({\n",
    "        \"email_id\": state.get('email_id',''),\n",
    "        \"original_email\": state.get('email_content',''),\n",
    "        \"draft_response\": state.get('draft_response',''),\n",
    "        \"urgency\": classification.get('urgency'),\n",
    "        \"intent\": classification.get('intent'),\n",
    "        \"action\": \"Please review and approve/edit this response\"\n",
    "    })\n",
    "\n",
    "    # Now process the human's decision\n",
    "    if human_decision.get(\"approved\"):\n",
    "        return Command(\n",
    "            update={\"draft_response\": human_decision.get(\"edited_response\", state.get('draft_response',''))},\n",
    "            goto=\"send_reply\"\n",
    "        )\n",
    "    else:\n",
    "        # Rejection means human will handle directly\n",
    "        return Command(update={}, goto=END)\n",
    "\n",
    "def send_reply(state: EmailAgentState) -> dict:\n",
    "    \"\"\"Send the email response\"\"\"\n",
    "    # Integrate with email service\n",
    "    print(f\"Sending reply: {state['draft_response'][:100]}...\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f4ee1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import RetryPolicy\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(EmailAgentState)\n",
    "\n",
    "# Add nodes with appropriate error handling\n",
    "workflow.add_node(\"read_email\", read_email)\n",
    "workflow.add_node(\"classify_intent\", classify_intent)\n",
    "\n",
    "# Add retry policy for nodes that might have transient failures\n",
    "workflow.add_node(\n",
    "    \"search_documentation\",\n",
    "    search_documentation,\n",
    "    retry_policy=RetryPolicy(max_attempts=3)\n",
    ")\n",
    "workflow.add_node(\"bug_tracking\", bug_tracking)\n",
    "workflow.add_node(\"draft_response\", draft_response)\n",
    "workflow.add_node(\"human_review\", human_review)\n",
    "workflow.add_node(\"send_reply\", send_reply)\n",
    "\n",
    "# Add only the essential edges\n",
    "workflow.add_edge(START, \"read_email\")\n",
    "workflow.add_edge(\"read_email\", \"classify_intent\")\n",
    "workflow.add_edge(\"send_reply\", END)\n",
    "\n",
    "# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9188bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import RetryPolicy\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(EmailAgentState)\n",
    "\n",
    "# Add nodes with appropriate error handling\n",
    "workflow.add_node(\"read_email\", read_email)\n",
    "workflow.add_node(\"classify_intent\", classify_intent)\n",
    "\n",
    "# Add retry policy for nodes that might have transient failures\n",
    "workflow.add_node(\n",
    "    \"search_documentation\",\n",
    "    search_documentation,\n",
    "    retry_policy=RetryPolicy(max_attempts=3)\n",
    ")\n",
    "workflow.add_node(\"bug_tracking\", bug_tracking)\n",
    "workflow.add_node(\"draft_response\", draft_response)\n",
    "workflow.add_node(\"human_review\", human_review)\n",
    "workflow.add_node(\"send_reply\", send_reply)\n",
    "\n",
    "# Add only the essential edges\n",
    "workflow.add_edge(START, \"read_email\")\n",
    "workflow.add_edge(\"read_email\", \"classify_intent\")\n",
    "workflow.add_edge(\"send_reply\", END)\n",
    "\n",
    "# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cd0f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human review interrupt:[Interrupt(value={'email_id': 'email_123', 'original_email': 'I was charged twice for my subscription! This is urgent!', 'draft_response': '', 'urgency': 'critical', 'intent': 'billing', 'action': 'Please review and approve/edit this response'}, id='81855674afbd512eb05d4a1736b83d75')]\n",
      "Sending reply: We sincerely apologize for the double charge. I've initiated an immediate refund......\n",
      "Email sent successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test with an urgent billing issue\n",
    "initial_state = {\n",
    "    \"email_content\": \"I was charged twice for my subscription! This is urgent!\",\n",
    "    \"sender_email\": \"customer@example.com\",\n",
    "    \"email_id\": \"email_123\",\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "# Run with a thread_id for persistence\n",
    "config = {\"configurable\": {\"thread_id\": \"customer_123\"}}\n",
    "result = app.invoke(initial_state, config)\n",
    "# The graph will pause at human_review\n",
    "print(f\"human review interrupt:{result['__interrupt__']}\")\n",
    "\n",
    "# When ready, provide human input to resume\n",
    "from langgraph.types import Command\n",
    "\n",
    "human_response = Command(\n",
    "    resume={\n",
    "        \"approved\": True,\n",
    "        \"edited_response\": \"We sincerely apologize for the double charge. I've initiated an immediate refund...\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Resume execution\n",
    "final_result = app.invoke(human_response, config)\n",
    "print(f\"Email sent successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
